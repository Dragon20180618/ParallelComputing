核函数外部的并行：
1.核函数计算与数据传输之间的并行
2.主机计算与数据传输之间的并行
3.不同的数据传输之间的并行
4.核函数计算与主机计算之间的并行
5.不同核函数之间的并行

创建一个CUDA流
cudaError_t cudaStreamDestory(cudaStream_t *);

cudaStream_t stream_1;
cudaStreamCreate(&stream_1); // 注意要传流的地址
cudaStreamDestroy(stream_1);
错误方法：
	cudaStream_t *stream_1;
	cudaStreamCreate(stream_1); // 注意要传流的地址
	cudaStreamDestroy(*stream_1);

等待流结束
cudaError_t cudaStreamSynchronize(cudaStream_t stream);
检查流是否完成，完成返回cudaSuccess，否则返回cudaErrorNotReady
cudaError_t cudaStreamQuery(cudaStream_t stream);

同一个 CUDA 流中的 CUDA 操作在设备中是顺序执行的，
故同一个 CUDA 流中的核函数也必须在设备中顺序执行
但主机发出一个核函数调用的命令之后都立刻获得程序控制权

调用核函数的三种方式
my_kernel<<<N_grid, N_block>>>(函数参数);
my_kernel<<<N_grid, N_block, N_shared>>>(函数参数);
my_kernel<<<N_grid, N_block, N_shared, stream_id>>>(函数参数);

如果要使用非默认流，不可以省去参数N_shared, 如果不需要共享内存，可以传0
my_kernel<<<N_grid, N_block, 0, stream_id>>>(函数参数); // 正确
不能用如下调用方式：
my_kernel<<<N_grid, N_block, stream_id>>>(函数参数); // 错误

单个GPU中能够并发执行的核函数个数的上限：
在计算能力为 
3.0、3.2、3.5、3.7、5.0、5.2、5.3、6.0、6.1、6.2、7.0 和 7.5 的 GPU 中，该上限的值分别为 
16、  4、  32、 32、 32、32、 16、128、32、16、128 和 128

异步传输由GPU中的DMA（Direct Memory Access直接内存访问）直接实现
cudaError_t cudaMemcpyAsync
(
	void *dst,
	const void *src,
	size_t count;
	enum cudaMemcpyKind kind,
	cudaStream_t stream
);
cudaMemcpyAsync之比cudaMemcpy多一个参数 cudaStream_t
关于enum，C++的派生类型，可以通过一些操作自动赋值
https://www.runoob.com/w3cnote/cpp-enum-intro.html

只有不可分页中的内存可以实现异步操作
申请不可分页内存的函数
	cudaError_t cudaMallocHost(void **dst, size_t size);
	cudaError_t cudaHostAlloc(void** dst, size_t size, size_t flags);
当cudaHostAlloc的第三个参数为cudaHostAllocDefault时，将等效于cudaMallocHost

释放函数：
cudaError_t cudaFreeHost(void* dst);
如果错用free()函数，将导致运行时错误

两个流中的执行方式
Stream 1：H2D -> KER -> D2H
Stream 2：            H2D -> KER -> D2H
可见两个流中的 H2D 操作不能并发地执行（受硬件资源的限制）
      两个流中的D2H操作同样不能并发的执行
如果上述中的单项操作时间相同，则并行后的加速比为6/4=1.5被

Stream 1：H2D -> KER -> D2H
Stream 2：            H2D -> KER -> D2H
Stream 3：                         H2D -> KER -> D2H
Stream 4：                                    H2D -> KER -> D2H
可见，虽然两个流中的H2D或D2H都不能并发，但H2D可以和D2H并发
